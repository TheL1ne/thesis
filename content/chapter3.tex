\chapter{State of the Art}
\section{Aktueller Trend in industriellen Netzwerken}
Der Großteil der Netzwerke im industriellen Kontext wird auch heute noch speziell für eine Fertigungsanlage geplant. Der Grund dafür sind spezielle Anforderungen an die Netzwerkhardware um Teilnehmern bestimmte Kommunikationszeiten (sogenannte Slots) und Bandbreiten garantieren zu können. Doch durch das aufkommen von Streamingdiensten in der Unterhaltungselektronik sind die Anforderungen an Stabilität, Verfügbarkeit und gleichzeitiger steigender Mobilität auch im Endkundenbereich gestiegen. Die damit verbundene Qualitätssteigerung in den Netzen und Netzwerkhardware bringt Verbrauchernetzwerktechnologie in den Blickpunkt von Forschungsarbeiten mit dem Ziel diese besser für Echtzeitnahe industrielle Anwendungen nutzen zu können\cite{wollschlaeger2017future}. Das Ziel dabei ist einfach: Geld einsparen. Besonders interessant ist die bewiesene Fehlerarmut der Endkundentechnologien, da sie durch viele Millionen Nutzer intensiveren Tests unterzogen werden, als es Testingenieure in annehmbarer Zeit durchführen könnten.\\
Deswegen sieht die aktuelle Entwicklung der Industrie vor neuen mobilen Endgeräte wie Smartphones, Tablets oder Laptops als Endpunkte für die Darstellung von internen Daten zu nutzen. Dabei sollen aber auch die bereits vorhanden Geräte der Mitarbeiter genutzt werden\cite{french2014current}. In Kombination mit den dabei steigenden Anzahl an Netzwerkteilnehmern und kabellosen Geräten, sinkt auch die Kontrolle über eben jene. Außerdem werden auch immer mehr kabellose Sensoren und komplexere Subgeräte eingebaut, da eine steigende Komplexität von Bauteilen die Kosten der Herstellung in die Höhe treibt, wird mit flexibleren Nutzungsmöglichkeiten versucht entgegen zu wirken. So entstand ein neues Forschungsfeld: \acrlong{cps}. Dieses Forschungsfeld führt unterschiedlichste Technologien zusammen und ist ein Kernelement der \textit{vierten industriellen Revolution}.\\
Die vierte industrielle Revolution oder \textit{\Gls{Industrie40}} wie es auch in vielen Veröffentlichungen genannt wird, konzentriert sich nun auf die massive Ausweitung der Kommunikation zwischen Maschine und Maschine (\Gls{M2M}) und Produkt und Maschine \cite{lasi2014industry}. Die Zukunftsvision sieht dabei vor, dass die Produkte ihren Zustand und die Schritte die noch notwendig sind um sie nach Kundenwunsch fertig zu stellen, kennen. Die notwendigen Ressourcen (wie z.B. Maschinen und Rohstoffe) werden dann durch Prozesssteuerung zugewiesen. Dieses Prozesssteuerung spielt dabei eine so große Rolle, dass sie als kritische Komponente permanent verfügbar sein muss und deswegen mit passenden Vorgaben (wie \textit{\Gls{gracefulshutdown}} und \textit{\Gls{rollingupdates}}) in virtuellen Systemen von Cloud-Anbietern ausgerollt wird. Damit erreicht wird auf der einen Seite eine bessere Planbarkeit der eigenen IT-Kosten und andererseits eine stärkere Abstraktion von der Serverhardware erreicht. Das ist erstrebenswert, da es die globale Nutzung Endpunktes einfacher macht\cite{wollschlaeger2017future}. Beide Entwicklungen führen zu einer deutlichen Zunahme der Netzwerkkommunikation in Fertigungsanlagen und setzen ein verlässliches „Wide Area Network“ vorraus.\\

TODO: Lean-Automation -> einarbeiten

\section{Anomalieerkennung in industriellen Netzwerken}
\subsection{Grundlagen}
Der Begriff der Anomalie ist im Zusammenhang mit industriellen Netzwerken sehr breit angelegt und die Erkennung einer Anomalie kann deswegen auch auf unterschiedliche Arten angegangen werden. Im Allgemeinen ist eine Anomalie einfach eine Abweichung vom Normal- oder Sollzustand und kann sich je nach Betrachtungsebene unterschiedlich manifestieren. Daher unterscheiden \textit{Ahmed et al.}\cite{ahmed2016survey} zwischen drei allgemeinen Klassen von Anomalien:
\begin{itemize}
\item \textit{Punktanomalie} ist vorhanden, wenn eine einzelne Datenquelle sich vom üblichen Muster des Datensatzes unterscheidet.
\item Wohingegen eine \textit{Kontextanomalie} sich darin äußert, dass eine Datenquelle sich nur unter bestimmten Bedingungen, also in einem bestimmten Kontext, von üblichen Muster unterscheiden.
\item \textit{Kollektivanomalie} bezeichnet schließlich eine Abweichung vieler ähnlicher Datenquellen vom üblichen Muster.
\end{itemize}
Neben der Art einer Anomalie können die Ergebnisse von Erkennungslagorithmen auf zwei Arten vorliegen: \textit{\Glspl{score}} und \textit{\Glspl{label}}. \textit{\Gls{score}} oder auch Bewertungen geben an, wie wahrscheinlich ein Zustand von einem Wünschenswerten abweicht. Wir erhalten als Ergebnisse üblicherweise Werte im Bereich zwischen 0 und 1. Dem gegenüber stehen die \textit{\Glspl{label}}, die eine binäre Aussage treffen, ob eine Anomalie vorhanden ist. Dabei kann ein \textit{\Gls{score}} zu einem \textit{\Gls{label}} werden, wenn ein Schwellwert definiert wird bei dem davon ausgegangen wird, dass eine Anomalie vorhanden ist. \cite[p. ~22]{ahmed2016survey}\\
Eine der Ursachen einer Anomalie kann das Eindringen eines nicht autorisierten Nutzers zu internen Netzwerkressourcen sein. Deswegen definiert \textit{Ahmed et al.} \cite{ahmed2016survey} vier Angriffsszenarien die zu Netzwerkanomalien führen können:
\begin{enumerate}
\item Bei \textit{Denial-of-Service}-Angriffen(DoS) werden Netzwerkdienste mit einer plötzlichen Hohen Anzahl von Anfragen geflutet, wodurch Server überlastet werden und reguläre Nutzeranfragen nicht mehr beantwortet werden können.
\item \textit{Probing} dient dazu Informationen zu sammeln. Dabei haben es Angreifer auf Details zu installierter Software und genutzter Infrastruktur (wie Anzahl und Typ der eingesetzten Maschinen). Es ist in vielen Fällen der erste Schritt für folgende Angriffe.
\item \textit{User to Roots}(U2R) bezeichnet einen Angriff bei dem ein Nutzer versucht seine Rechte auf Ressourcen auszudehnen auf die er keinen Zugriff haben darf. Dies kann durch das ausspähen von Passwörtern von Admin-Konten oder ausnutzen von Systemverwundbarkeiten passieren.
\item \textit{Remote to User}(R2U): hier handelt es sich um den Versuch eine bestimmte Instanz des Zielnetzwerks zu übernehmen um über diese dann weitere Netzwerkpakete einschleusen zu können.
\end{enumerate}
TODO:Bild Seite 23

\subsection{Erkennungsalgorithmen}
Die Berechnung dieser Werte wurde in den letzten Dekaden intensiv untersucht und so haben sich unterschiedliche Klassen von Ansätzen herausgebildet\cite{ahmed2016survey}:
\begin{itemize}
\item \textit{Statistische Ansätze} basieren ebenfalls auf einem Normal-Profil des Systems in Form der Wahrscheinlichkeitsverteilung für zukünftige Ereignisse basierend auf den bereits bearbeiteten  Ereignissen vorhanden ist. Dabei nutzen \textit{Ye et al.} \cite{ye2001anomaly} die \textit{Chi-Square-Formel} zur Berechnung der Abweichung eines Ereignises von der erwarteten Abfolge. Damit ist es auch möglich seltene Punktanomalien in massiven Datensätzen aufzuspüren.
\item \textit{Informationstheoretische Ansätze} basieren auf Kenngrößten der Informationtheorie wie \textit{(relativer und/oder bedingter) Entropie} und Informationsgewinn eines Datensatzes. Dabei ist es notwendig Trainingsdaten bereit zu stellen um die Kenngrößen für den vorliegenden Fall bestimmen zu können. \cite{noble2003graph}
\item Bei den \textit{Clusterbasierte Ansätze} wie \textit{k-Means}\cite{likas2003global} werden Messwerte bezüglich eines passend gewähltem Koordinatensystem eingetragen. Danach können die einzelnen Punkte zu Gruppen zusammengefasst werden und je nach Dichte und Ausdehnung als Anomalien oder reguläres Verhalten eingestuft werden. Diesem Ansatz liegt die Annahme zu Grunde, dass normales Verhalten häufig vorkommt und die zugehörigen Messwerte eng bei einander liegen. In vielen Fällen ist dafür eine Vorverarbeitung der Messungen notwendig \cite{munz2007traffic}. Dafür ist es möglich ein Modell zur Klassifikation von einem Datensatz ohne Markierung der Messpunkte (ob Anomalie oder nicht) zu erhalten. Man spricht dabei von \textit{Unsupervised Machine Learning}\cite{munz2007traffic}.
\item \textit{Klassifikationsbasierte Ansätze} konzentrieren sich vor allem auf die Detektierung von Netzwerkattacken und basieren dabei entweder auf \Glspl{signatur} oder einem Profil des normalen Netzwerkverkehrs. Diese\Glspl{signatur} werden dabei durch Experten festgelegt und dem System bekannt gemacht. Dadurch können diese Ansätze nur bekannte Attacken detektieren und sind anfällig für neue oder individuell zugeschnittene Angriffe. Bei den profilbasierten Klassifikationen muss ein Profil entweder erst gelernt oder hinterlegt werden. Es wird dann davon ausgegangen, dass sämtliche Anfragen außerhalbs des vorhandenen Profils eine Anomalie darstellt. Da es allerdings unrealistisch ist ein erschöpfendes Profil der erlaubten Anfragen zu hinterlegen, kann dieser Ansatz zu \Glspl{falsepositive} führen. 
\end{itemize}
Diese Ansätze sind nicht völlig voneinander trennbar, denn häufig werden Verfahren wie \textit{Clustering} auch außerhalb der \textit{Clusterbasierten Ansätze} genutzt. So verwendet \textit{Eskin et al.} \cite{eskin2002geometric} Clusteralgorithmen zur Detektion von Ausreißern nachdem die Daten auf wichtige Merkmale untersucht und in einem virtuellen Koordinatensystem aus Merkmalvektoren aufgeteilt wurden. Außerdem lassen sich die oben genannten Klassen der Ansätze noch weiter unterteilen. Beispielweise fallen die folgenden Technologien unter die \textit{Klassifikationsbasierten Ansätze}: \textit{Support Vector machine} \cite{noble2006support}, \textit{Bayesian Network} \cite{kruegel2003bayesian}, \textit{Neurale Netzwerke} \cite{zhang2001hide}, Regelbasierte Ansätze \cite{yang2013rule}.

\section{Ontologien und Anomalieerkennung}
\subsection{Der Begriff Ontologie}
Der Begriff der \Gls{Ontologie} entstammt der Philosophie und entspricht dort dem Teil der Philosophie der sich mit \textit{der Natur der Welt und der Struktur der Realität} befasst. Dabei wird untersucht welche Eigenschaften  Gegenständen, Objekten und Lebewesen zugeordnet aufgrund ihrer Natur zugeordnet werden können. Dabei setzen manche Philosophen \Gls{Ontologie} und Metaphysik gleich. \cite{guarino2009ontology}.\\
Der Begriff \Gls{Ontologie} wurde dann im Verlauf der 1970er während der Erforschung unf Formalisierung von Beschreibungslogik in die Informatik übernommen\cite{roy2010exploitation}. \Glspl{Ontologie} im Sinne der Informatik werden als \textit{formelle Spezifikation einer geteilten Begriffsbildung} definiert (englisches Original: \textit{formal speciﬁcation of a shared conceptualization}\cite{borst1999construction}). Es wird dabei analog zur philosophischen Disziplin ein System mit allen relevanten \textit{Entitäten} und deren Beziehungen zu einander formal definiert. Der grundlegender Teil einer Ontologie sind daher zwei Taxonomiehierarchien: eine für Konzepte und eine für Relationen. So erbt jedes Konzept einer Taxonomie von \textit{Ding} und ist damit immer eine Spezialisierung eines \textit{Dings}.\\
Ein weiterer Teil einer Ontologie sind \textit{Individuen}, welche als Instanziierung eines \textit{Konzepts} angesehen werden und damit die formale Repräsentation der Anwendung eines Konzepts darstellt. Eine Taxonomie wird dabei zu einer Ontologie, wenn die definierten Konzepte mit Hilfe der definierten Relationen in Beziehungen gesetzt werden. Bemerkernswert ist dabei die \textit{Open-World-Assumption}: die Annahme, dass alles nicht explizit ausgeschlossene oder auch nicht definierte Wissen existieren kann\cite{razniewski2016turning}. Aus diesem Grund ist besondere Vorsicht beim Entwurf von Ontologien geboten.

\subsection{Anwendung von Ontologien}
2001 definierten \textit{Berners-Lee et al.}\cite{berners2001semantic} eine neue Art des \textit{World Wide Web}: \textit{the Web of Data} oder auch \textit{Semantic Web}. Dabei wurde die Idee geboren Maschinen zu ermöglichen die Bedeutung von Daten im Web zu verstehen um die Effizienz von Menschen im Umgang mit diesen Daten zu verbessern\cite{lasi2014industry}. Dazu wurden \Glspl{Ontologie} genutzt um Taxonomien und Schlussfolgerungsregeln abzubilden. Zur Formalisierung wurde ein neues Framework definiert: das \textit{\Gls{rdf}} um optimierte Suchen zu ermöglichen und so die Trefferanzahl von Suchmaschinen zu erhöhen\cite{feilmayr2016analysis}. \textit{\Gls{rdf}} folgt dabei einer einfachen Syntax von \textit{Subjekt-Prädikat-Objekt}. Aus diesen Tripeln von Aussagen können Inferenzmaschinen oder auch \textit{Reasoner} genannt Aussagen ableiten, wie beispielsweise dass jeder Vater auch ein Mann und damit eine Person ist\cite{roy2010exploitation}. Die Hauptoperation dabei ist das Zusammenfassen oder Klassifizieren (engl. original \textit{to subsume}. Mit Hilfe dieser Operation können definierte Ontologien auf Konsistenz und die Abwesenheit von Widersprüchen geprüft werden. Außerdem ist es möglich konzeptuelle Erfüllbarkeit nachzuweisen, also ob für eine Klasse Instanzen existieren können. Des weiteren könne weitere Aussagen über Vererbungsrelationen getroffen werden ohne diese explizit in der Taxonomie definiert zu haben und leitet eine Inferenzmaschine für jedes Individuum das Konzept ab, dass die höchste Spezialisierung aufweist. Individuen werden also typisiert anhand ihrer Merkmale und Relationen\cite{roy2010exploitation}.\\
Diese Art der Typisierung wurde bereits zur Anomalieerkennung in einem maritimen Kontext benutzt um Schiffe zu klassifizieren. Ziel war es interessante Vorfälle aus einem großen Bestand an maritimen Verkehrsdaten heraus zu lösen und so Vorfälle von Piraterie oder Wilderei (beispielsweise durch das \textit{Verschwinden} vom Schiffen vom Datensatz) aufzuspüren\cite{roy2010exploitation}. Dabei wurde das Domainwissen als Ontologiemodell abgebildet und als Teil einer statischjen A-Priori-Wissensdatenbank genutzt. Danach wurden situative Fakten mit dieser Wissensdatenbank via Inferenzmaschine verschmolzen und so Anomalien gefunden.\\
Bei dieser Arbeit wurde \textbf{Pellet}\cite{pellet} als \Gls{reasoner} genutzt, da \textbf{Pellet} sowohl kommerziell genutzt wird als auch eine breite Nutzerbasis im Open-Source-Bereich aufweist. Im Verlauf der Evaluation kamen \textit{Roy et al.} zu dem Ergebnis, dass der kontinuierliche Modus, also das einzelne Hinzufügen von Situationsdaten, viel zu langsam ist und entschieden sich deswegen für den \textit{Batch}-Modus. Das Problem ist, dass im kontinuierlichen Modus der \textit{Reasoner} nach jeder Addition eines neuen Faktes neu klassifiziert und dabei auch die geschlussfolgerte Hierarchie der Individuen neu evaluiert\cite[p.~4]{roy2010exploitation}.\\